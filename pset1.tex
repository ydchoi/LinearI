\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\def\eQb#1\eQe{\begin{eqnarray*}#1\end{eqnarray*}}
\def\eQnb#1\eQne{\begin{eqnarray}#1\end{eqnarray}}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\providecommand{\pb}[0]{\pagebreak}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\def\Qb#1\Qe{\begin{question}#1\end{question}}
\def\Sb#1\Se{\begin{solution}#1\end{solution}}

\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{question}{Question}
\newtheorem*{preposition}{Preposition}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newtheorem*{solution}{Solution}
\newtheorem*{remark}{Remark}
\usepackage{verbatimbox}
\usepackage{listings}
\title{Linear Algebra I: \\
Problem Set I}


\author{
Youngduck Choi \\
CIMS \\
New York University\\
\texttt{yc1104@nyu.edu} \\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
This work contains the solutions to the problem set I
of Linear Algebra I 2015 at Courant Institute of Mathematical Sciences.
\end{abstract}

\bigskip

\begin{question}[1]
\end{question}
\begin{solution}
Let $u,v,w$ be a basis for a three dimensional vector space $V$. 
We show that the three vectors $u+v+w$, $v+w$, and $w$ are linearly 
independent. Assume that 
\eQb
a_1(u+v+w) + a_2(v+w) + a_3(w) &=& 0.
\eQe
Rearranging yields
\eQb
(a_1)u + (a_1+a_2)v + (a_1+a_2+a_3)w &=& 0.
\eQe
As $u,v,w$ form a basis, they are linearly independent. Hence, we have 
\eQb
a_1 &=& 0 \\
a_1 + a_2 &=& 0 \\
a_1 + a_2 + a_3 &=& 0. \\
\eQe
Solving the system yields
\eQb
a_1 &=& 0 \\
a_2 &=& 0 \\
a_3 &=& 0. \\
\eQe
Hence, the three vectors, $u+v+w$, $v+w$ and $w$ are linearly independent. Now,
let $v \in V$. As $u,v,w$ is a basis for $v$, $v$ can be written as
\eQb
v &=& c_1u + c_2v + c_3w.
\eQe
The above equality can be re-expressed as
\eQb
v &=& c_1(u+v+w) + (c_2 - c_1)(v+w) + (c_3 - c_2)w. 
\eQe
Therefore $v$ can be written as a linear combination of $u+w+v$, $v+w$ and $w$.
Since $v$ was arbitrary, we have shown that $u+v+w, v+w$ and $w$ span $V$.
Therefore, $u+v+w, v+w$ and $w$ form a basis of $V$.
$\qed$
\end{solution}

\pagebreak

\begin{question}[2]
\end{question}
\begin{solution}

\end{solution}


\bigskip

\begin{question}[3]
\end{question}
\begin{solution}
Consider the following two pairs of reals: $(1,1)$ and $(2,2)$.
Then, by the given definition of addition, we have
\eQb
(1,1) + (2,2) &=& (5,7) \\
(2,2) + (1,1) &=& (4,5). \\
\eQe 
Hence, we have that $(1,1) + (2,2) \neq (2,2) + (1,1)$. The given
addition fails to be commutative. Therefore,
the given set of pairs do not form a vector space under the given
definitions. $\qed$
\end{solution}

\bigskip

\begin{question}[4]
\end{question}
\begin{solution}
Let $W_1$ and $W_2$ be subspaces of $V$. First, assume that
$W_1 \subseteq W_2$. Then, we have $W_1 \cup W_2 = W_2$. Since
$W_2$ is a subspace, we have that $W_1 \cup W_2$ is a subspace. By symmetry, we also have that if $W_2 \subseteq W_1$, then $W_1 \cup W_2$ is a subspace.

\smallskip

Now, assume that $W_1 \cup W_2$ is a subspace. 
Suppose for sake of contradiction that $W_1 \nsubseteq W_2$ and
$W_1 \nsubseteq W_2$. 
either $W_1 \setminus W_2 \neq \emptyset$ or  
Let $x \in W_1$ and $y \in W_1 \setminus W_2$.


Hence, we have shown that $W_1 \cup W_2$ is a subspace iff $W_1 \subseteq W_2$
or $W_2 \subseteq W_1$.  

\end{solution}

\bigskip

\begin{question}[5]
\end{question}
\begin{solution}
Let $p_1(x)$ and $p_2(x) \in P_1(\mathbb{R})$. First, note that
polynomials are integrable, hence the integrals are well-defined.
By the linearity of integration, we have
\eQb
f_1(p_1(x) + p_2(x)) &=& \int_{0}^{1} p_1(t) + p_2(t) dt \\
&=& \int_{0}^{1} p_1(t) dt + \int_{0}^{1} p_2(t) dt \\
&=& f_1(p_1(x)) + f_1(p_2(x)), \\
f_2(p_1(x) + p_2(x)) &=& \int_{1}^{2} p_1(t) + p_2(t) dt \\
&=& \int_{1}^{2} p_1(t) dt + \int_{1}^{2} p_2(t) dt \\
&=& f_2(p_1(x)) + f_2(p_2(x)). \\
\eQe
For a scalar $\alpha$ and $p(x) \in P_1(\mathbb{R})$,
by the linearity of integration again,
\eQb
f_1(\alpha p(x)) &=& \int_{0}^{1} \alpha p(t) dt \\
&=& \alpha \int_{0}^{1} p(t) dt \\
&=& \alpha f_1(p(x)), \\
f_2(\alpha p(x)) &=& \int_{1}^{2} \alpha p(t) dt \\
&=& \alpha \int_{1}^{2} p(t) dt \\
&=& \alpha f_2(p(x)).
\eQe
Therefore, both $f_1$ and $f_2$ are linear functionals and are in the dual of $P_1(\mathbb{R})$.

\smallskip

We now claim that $f_1$ and $f_2$ are linearly independent. Assume that
\eQb
a_1 f_1 + a_2 f_2 &=& 0,
\eQe
Hence $a_1, a_2 = 0$. Therefore, we have shown that 

Since we know that $dim P_1(\mathbb{R}) = 2$, and th
\end{solution}

\bigskip

\begin{question}[6]
\end{question}
\begin{solution}
Let $V$ be a vector space and $x_1$ be a nonzero vector in $V$. Let
$V'$ be the dual of $V$. We argue that an identity map $I$, which is
defined by $I(x) = x \> \forall x \in V$, is in $V'$.
Let $x,y \in V$. Then, we have
\eQb
I(x+y) &=& x + y \\
&=& I(x) + I(y). \\
\eQe
Since $x,y$ were arbitrary, we $I(x+y) = I(x) + I(y)$ for all $x,y$.
Now, let $k$ be a scalar and $x \in V$. Then, we have
\eQb
I(kx) &=& kx \\
&=& kI(x). \\
\eQe
Since $k,x$ were arbitrary, we have $I(kx) = kI(x)$ for all $x \in V$
and all scalars. Therefore, $I$ is linear and is in $V'$. Notice that
$I(x_1) = x_1 \neq 0$. Hence, we have found a map where $x_1$
mapped to a non-zero element. $\qed$  
\end{solution}

\bigskip

\begin{question}[7]
\end{question}
\begin{solution}
We wish to show that the annihilator $Y^{\bot}$ is a subspace of the dual $V'$.
Let $l_1 , l_2 \in Y^{\bot}$, and consider 
$l_1 + l_2$.  Let $y \in Y$.
Then, by definition of the annihilator, we have
\eQb
l_1+l_2(y) &=& l_1(y) + l_2(y) \\ 
&=& 0. \\
\eQe 
Since $y$ was arbitrary, we have 
\eQb
l_1 + l_2(y) = 0 \>\> \forall y \in Y. 
\eQe
Therefore, $l_1 + l_2$ is in $Y^{\bot}$. \\

\smallskip

Now, let $l \in Y^{\bot}$, and consider $\alpha l$, where
$\alpha$ is a scalar. Let $ y \in Y.$ 
Then, again by definition of the annihilator, we have
\eQb
\alpha l(y) &=& (\alpha)l(y) \\
&=& (\alpha)(0) \\
&=& 0. \\
\eQe
Since $y$ was arbitrary, we have
\eQb
\alpha l(y) = 0 \>\> \forall y \in Y.
\eQe
Therefore, $\alpha l$ is in $Y^{\bot}$.
As we have shown that $Y^{\bot}$ is closed under addition and
scalar multiplication,
we have shown that $Y^{\bot}$ is a subspace of the dual $V'$. $\qed$

\end{solution}

\bigskip

\begin{question}[8]
\end{question}
\begin{solution}
Let $V$ be a finite dimensional vector space with two different bases
$\{ x_1, ..., x_n \}$ and $\{ y_1, ..., y_n \}$. Let $v \in V$ and
denote the cordinates of $v$ with respect to the $x$ basis and
with respect to the $y$ basis respectively as
\eQnb
v &=& \sum_{i=1}^{n} a_i x_i \\ 
v &=& \sum_{i=1}^{n} b_i y_i. 
\eQne 
Now, observe that each $y_i$ vector from the $y$ basis as a determined
cordinates with respect to the $x$ basis. We write them as follows:
\eQb
y_i = \sum_{k=1}^{n} c_{ik}x_k,
\eQe
for $1 \leq i \leq n$. Now substituting the above equality into $(2)$,
we obtain
\eQb
v &=& \sum_{i=1}^{n} (b_i\sum_{k=1}^{n} c_{ik}x_k),
\eQe
which can be re-written as
\eQb
v &=& \sum_{k=1}^{n}(\sum_{i=1}^{n} b_i c_{ik}) x_k.
\eQe
Hence, by matching the coefficients of the above equality to $(1)$ 
and agreeing the dummy variables for indices,
we have
\eQb
a_i &=& \sum_{k=1}^{n} b_k c_{ki},
\eQe
for $1 \leq i \leq n$. This deduction establishes the 
required relation as desired. $\qed$
\end{solution}

\end{document}
